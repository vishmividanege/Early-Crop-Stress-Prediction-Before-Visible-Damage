{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56bde1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vishm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rasterio\\__init__.py:368: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRADIENT BOOSTING REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       133\n",
      "           1       1.00      1.00      1.00       307\n",
      "\n",
      "    accuracy                           1.00       440\n",
      "   macro avg       1.00      1.00      1.00       440\n",
      "weighted avg       1.00      1.00      1.00       440\n",
      "\n",
      "Confusion Matrix:\n",
      " [[132   1]\n",
      " [  0 307]]\n",
      "Gradient Boosting model saved\n",
      "Gradient Boosting Accuracy\n",
      "Train: 1.0\n",
      "Test : 0.9977272727272727\n",
      "Model created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vishm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rasterio\\__init__.py:368: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.2919\n",
      "Epoch 2/10 - Loss: 0.1746\n",
      "Epoch 3/10 - Loss: 0.1962\n",
      "Epoch 4/10 - Loss: 0.1969\n",
      "Epoch 5/10 - Loss: 0.2220\n",
      "Epoch 6/10 - Loss: 0.2133\n",
      "Epoch 7/10 - Loss: 0.1922\n",
      "Epoch 8/10 - Loss: 0.1668\n",
      "Epoch 9/10 - Loss: 0.2016\n",
      "Epoch 10/10 - Loss: 0.1715\n",
      "=== CNN REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93       140\n",
      "           1       0.97      0.97      0.97       300\n",
      "\n",
      "    accuracy                           0.96       440\n",
      "   macro avg       0.95      0.95      0.95       440\n",
      "weighted avg       0.96      0.96      0.96       440\n",
      "\n",
      "Confusion Matrix:\n",
      " [[131   9]\n",
      " [ 10 290]]\n",
      "CNN model saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from torchvision import models\n",
    "\n",
    "# ----------------------\n",
    "# Device & CSV\n",
    "# ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "df = pd.read_csv(\"pseudo_labels_with_exg.csv\")\n",
    "file_names = df[\"image\"].tolist()\n",
    "labels = df[\"pseudo_label\"].tolist()\n",
    "base_dir = r\"E:\\Early Crop Stress Prediction Before Visible Damage\\Dataset\"\n",
    "\n",
    "# ----------------------\n",
    "# Helper function: read tif\n",
    "# ----------------------\n",
    "def read_tif(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        img = src.read(1)\n",
    "    img = cv2.resize(img, (128, 128)).astype(np.float32)\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "    return img\n",
    "\n",
    "# ----------------------\n",
    "# Gradient Boosting features\n",
    "# ----------------------\n",
    "features = []\n",
    "for _, row in df.iterrows():\n",
    "    rgb = cv2.imread(os.path.join(base_dir, \"RGB\", row[\"image\"]))\n",
    "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "    rgb = cv2.resize(rgb, (128,128)).astype(np.float32)/255.0\n",
    "\n",
    "    R, G, B = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    exg = 2*G - R - B\n",
    "\n",
    "    ndvi = read_tif(os.path.join(base_dir, \"NDVI\", row[\"image\"]))\n",
    "    vh   = read_tif(os.path.join(base_dir, \"SAR\",\"VH\",row[\"image\"]))\n",
    "    vv   = read_tif(os.path.join(base_dir, \"SAR\",\"VV\",row[\"image\"]))\n",
    "\n",
    "    features.append([ndvi.mean(), vh.mean(), vv.mean(), exg.mean()])\n",
    "\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# Gradient Boosting\n",
    "# ----------------------\n",
    "gb = GradientBoostingClassifier(n_estimators=150, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "\n",
    "print(\"=== GRADIENT BOOSTING REPORT ===\")\n",
    "print(classification_report(y_test, gb_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, gb_pred))\n",
    "\n",
    "joblib.dump(gb, \"gb_crop_stress_model1.pkl\")\n",
    "print(\"Gradient Boosting model saved\")\n",
    "\n",
    "print(\"Gradient Boosting Accuracy\")\n",
    "print(\"Train:\", accuracy_score(y_train, gb.predict(X_train)))\n",
    "print(\"Test :\", accuracy_score(y_test, gb.predict(X_test)))\n",
    "\n",
    "# ----------------------\n",
    "# Data Augmentation for CNN\n",
    "# ----------------------\n",
    "def augment_image(img):\n",
    "    \"\"\"Augment 6-channel image (RGB+NDVI+VH+VV)\"\"\"\n",
    "    img = img.astype(np.float32)\n",
    "    H, W, C = img.shape\n",
    "\n",
    "    # Horizontal flip\n",
    "    if random.random() > 0.5:\n",
    "        img = np.flip(img, axis=1).copy()  # <-- add .copy()\n",
    "    # Vertical flip\n",
    "    if random.random() > 0.5:\n",
    "        img = np.flip(img, axis=0).copy()  # <-- add .copy()\n",
    "    # Rotation -15 to 15 degrees\n",
    "    angle = random.uniform(-15, 15)\n",
    "    M = cv2.getRotationMatrix2D((W/2, H/2), angle, 1)\n",
    "    for c in range(C):\n",
    "        img[:,:,c] = cv2.warpAffine(img[:,:,c], M, (W,H), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "    # Random brightness on RGB only\n",
    "    if random.random() > 0.5:\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        img[:,:,:3] = np.clip(img[:,:,:3] * factor, 0, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Dataset Class\n",
    "# ----------------------\n",
    "class CropDataset(Dataset):\n",
    "    def __init__(self, base_dir, files, labels, augment=False):\n",
    "        self.base_dir = base_dir\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        rgb = cv2.imread(os.path.join(self.base_dir,\"RGB\",name))\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        rgb = cv2.resize(rgb,(128,128)).astype(np.float32)/255.0\n",
    "\n",
    "        ndvi = read_tif(os.path.join(self.base_dir,\"NDVI\",name))[...,None]\n",
    "        vh   = read_tif(os.path.join(self.base_dir,\"SAR\",\"VH\",name))[...,None]\n",
    "        vv   = read_tif(os.path.join(self.base_dir,\"SAR\",\"VV\",name))[...,None]\n",
    "\n",
    "        img = np.concatenate([rgb, ndvi, vh, vv], axis=2)\n",
    "\n",
    "        if self.augment:\n",
    "            img = augment_image(img)\n",
    "\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "        return img, torch.tensor(label)\n",
    "\n",
    "# ----------------------\n",
    "# Prepare DataLoaders\n",
    "# ----------------------\n",
    "dataset = CropDataset(base_dir, file_names, labels, augment=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "# ----------------------\n",
    "# CNN Model\n",
    "# ----------------------\n",
    "class ResNetCropStress(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        self.model.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 2)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = ResNetCropStress().to(device)\n",
    "print(\"Model created successfully\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# ----------------------\n",
    "# CNN Training Loop\n",
    "# ----------------------\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# CNN Evaluation\n",
    "# ----------------------\n",
    "model.eval()\n",
    "preds, true = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds.extend(torch.argmax(outputs,1).cpu().numpy())\n",
    "        true.extend(labels.numpy())\n",
    "\n",
    "print(\"=== CNN REPORT ===\")\n",
    "print(classification_report(true, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(true, preds))\n",
    "\n",
    "torch.save(model.state_dict(), \"cnn_crop_stress_model1.pth\")\n",
    "print(\"CNN model saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
